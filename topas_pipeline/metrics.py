import os.path
import os
import sys
import warnings
import logging
from pathlib import Path
from typing import List, Dict, Tuple, Union

import numpy as np
import pandas as pd

from . import clinical_annotation
from . import sample_annotation
from . import utils

# hacky way to get the package logger instead of just __main__ when running as python -m topas_pipeline.simsi ...
logger = logging.getLogger(__package__ + "." + __file__)

MEASURE_NAMES = [
    ("rank", "rank"),
    ("batchrank", "batchrank"),
    ("fc", "fold_change"),
    ("z", "z-score"),
]


def compute_metrics(
    results_folder: Union[str, Path],
    debug: bool,
    data_types: List[str],
    sample_annotation_file: Union[str, Path],
    measure_names: list[tuple[str, str]] = MEASURE_NAMES,
    overwrite: bool = False,
):
    """Computes rank, within batch rank, fold change and z-score
    Requires that annot_{data_type}.csv is generated by clinical_annotation.py
    """
    logger.info("Running compute metrics module")

    sample_annotation_df = sample_annotation.load_sample_annotation(
        sample_annotation_file
    )

    if "pp" in data_types:
        data_types.append("phospho_score")

    for data_type in data_types:
        if check_measures_computed(results_folder, data_type, measure_names):
            if not overwrite:
                logger.info(f"Skipping compute_metrics {data_type} - already computed")
                continue
            logger.info(f"Found existing results but overwrite flag was set.")

        logger.info(f"Reading in clinically annotated {data_type} data")
        annot_df = clinical_annotation.read_annotated_expression_file(
            results_folder, data_type
        )

        measures = get_metrics(
            utils.keep_only_sample_columns(annot_df), sample_annotation_df
        )

        save_measures(
            results_folder,
            measure_names,
            measures,
            data_type,
        )

        if debug:
            measures = get_metrics(
                utils.keep_only_sample_and_ref_columns(annot_df), sample_annotation_df
            )
            save_measures(
                results_folder, measure_names, measures, data_type + "_with_ref"
            )


def get_rank(df: pd.DataFrame) -> pd.DataFrame:
    """ """
    logger.debug("Calculating ranks")
    df_rank = df.copy()
    df_rank = df_rank.rank(ascending=False, method="average", na_option="keep", axis=1)
    # add new column that for each protein/peptide tells the max rank
    df_rank["max"] = df_rank.notnull().sum(axis=1)
    return df_rank.add_prefix("rank_")


def get_fold_change(df: pd.DataFrame) -> pd.DataFrame:
    """
    Leave-one-out approach (loo)
    """
    logger.debug("Calculating fold changes")
    df_fold_change = np.power(10, df)
    df_fold_change /= _loo_median(df_fold_change.values)
    df_fold_change = df_fold_change.add_prefix("fc_")
    return df_fold_change


def get_zscore(df: pd.DataFrame) -> pd.DataFrame:
    """
    Leave-one-out approach (loo)
    """
    logger.debug("Calculating z-scores")
    df_z_score = df.copy()
    df_z_score -= _loo_median(df.values)
    df_z_score /= _loo_std(df.values)
    df_z_score = df_z_score.add_prefix("zscore_")
    return df_z_score


def _loo_median(input_matrix: np.array) -> np.array:
    """
    Computes the leave-one-out median for each element row-wise.

    30 seconds for 100000 proteins and 2000 samples.
    """
    # 1. get the rank of each value within the row
    argsort_indices = np.argsort(input_matrix, axis=1)
    ranks = np.argsort(argsort_indices, axis=1)

    valid_vals = ranks.shape[1] - np.isnan(input_matrix).sum(axis=1)[:, np.newaxis]

    # 2. get the LOO median indices
    # lower_median and upper_median indices are the same if the LOO array has an odd number of elements
    lower_median = np.floor(
        valid_vals / 2  # index of median
        - (
            ranks >= valid_vals / 2
        )  # if the value is in the upper half, go one element down
        - (
            (ranks * 2 + 1) == valid_vals
        )  # if the value is exactly the median, go another element down
    ).astype(int)
    upper_median = np.ceil(valid_vals / 2 - (ranks >= valid_vals / 2)).astype(int)

    sorted_values = input_matrix[
        np.arange(input_matrix.shape[0])[:, np.newaxis], argsort_indices
    ]

    # reuse variable to (hopefully) save memory
    loo_median = sorted_values[
        np.arange(input_matrix.shape[0])[:, np.newaxis], lower_median
    ]
    loo_median += sorted_values[
        np.arange(input_matrix.shape[0])[:, np.newaxis], upper_median
    ]
    loo_median /= 2
    loo_median[np.isnan(input_matrix)] = np.nan

    return loo_median


def _loo_std(input_matrix: np.array) -> np.array:
    """LOO standard deviation using Welford's online algorithm reversed

    Args:
        input_matrix (np.array): _description_

    Returns:
        _type_: _description_
    """
    n = input_matrix.shape[1] - np.isnan(input_matrix).sum(axis=1)[:, np.newaxis]

    regular_sum = np.nansum(input_matrix, axis=1)[:, np.newaxis]
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=RuntimeWarning)
        regular_var = np.nanvar(input_matrix, axis=1)[
            :, np.newaxis
        ]  # uses ddof = 0 because that is the numpy default
    m2n = regular_var * n

    loo_sum = regular_sum - input_matrix
    loo_mean = loo_sum / np.clip(n - 1, 1, None)
    m2n_minus_1 = m2n - (input_matrix - loo_mean) * (
        input_matrix - regular_sum / np.clip(n, 1, None)
    )

    return np.sqrt(
        m2n_minus_1 / np.where(n - 2 >= 1, n - 2, np.nan)
    )  # uses ddof = 1 because that is the pandas default


def get_within_batch_rank(df: pd.DataFrame, sample_annotation_df: pd.DataFrame):
    sample_to_batch_mapping = sample_annotation_df["Batch Name"].copy()
    sample_to_batch_mapping.index = utils.add_patient_prefix(
        sample_to_batch_mapping.index
    )
    batch_rank_df: pd.DataFrame = df.T.groupby(
        by=df.columns.map(sample_to_batch_mapping),
    ).rank(method="min", ascending=False).T
    return batch_rank_df.add_prefix("batchrank_")


def get_metrics(
    df: pd.DataFrame, sample_annotation_df: pd.DataFrame
) -> Dict[str, pd.DataFrame]:
    """
    describe
    """
    logger.info("Calculating metrics")

    # Get metrics
    metrics_df = {}
    metrics_df["rank"] = get_rank(df)
    metrics_df["batchrank"] = get_within_batch_rank(df, sample_annotation_df)
    metrics_df["fold_change"] = get_fold_change(df)
    metrics_df["z-score"] = get_zscore(df)
    return metrics_df


def get_data_type_long(data_type: str):
    if data_type.startswith("pp"):
        return "phospho"
    elif data_type.startswith("fp"):
        return "full_proteome"
    elif data_type.startswith("phospho_score"):
        return "phospho_score"
    raise ValueError(f"Unknown data type {data_type}")


def check_measures_computed(
    results_folder: Union[str, Path],
    data_type: str,
    measure_names: List[Tuple[str]] = MEASURE_NAMES,
) -> Dict[str, pd.DataFrame]:
    """
    explain what modalities and measures have to be

    """
    data_type_long = get_data_type_long(data_type)
    for m, _ in measure_names:
        filename = os.path.join(results_folder, f"{data_type_long}_measures_{m}.tsv")
        if not os.path.exists(filename):
            return False

    return True


def read_measures(
    results_folder: Union[str, Path],
    data_type: str,
    measure_names: List[Tuple[str]] = MEASURE_NAMES,
) -> Dict[str, pd.DataFrame]:
    """
    explain what modalities and measures have to be

    """
    data_type_long = get_data_type_long(data_type)
    index_col = utils.get_index_cols(data_type)

    measures = dict()
    for m, measure in measure_names:
        filename = os.path.join(results_folder, f"{data_type_long}_measures_{m}.tsv")
        if not os.path.exists(filename):
            return dict()

        logger.info(f"Reading in {filename}")
        measures[measure] = pd.read_csv(filename, sep="\t", index_col=index_col)
    return measures


def save_measures(
    results_folder: Union[str, Path],
    measure_names: List[Tuple[str]],
    measures: Dict[str, pd.DataFrame],
    data_type: str,
):
    # save quantification measures
    data_type_long = get_data_type_long(data_type)

    filename_suffix = ""
    if data_type.endswith("_with_ref"):
        filename_suffix = "_with_ref"

    for m, measure in measure_names:
        filename = os.path.join(
            results_folder, f"{data_type_long}_measures_{m}{filename_suffix}.tsv"
        )
        measures[measure].to_csv(filename, sep="\t", float_format="%.6g")


if __name__ == "__main__":
    import argparse

    from . import config

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-c", "--config", required=True, help="Absolute path to configuration file."
    )
    parser.add_argument(
        "-o",
        "--overwrite",
        action="store_true",
        help="Ignore existing results and recompute outputs.",
    )
    args = parser.parse_args(sys.argv[1:])

    configs = config.load(args.config)

    compute_metrics(
        configs.results_folder,
        configs.preprocessing.debug,
        data_types=configs.data_types,
        sample_annotation_file=configs.sample_annotation,
        overwrite=args.overwrite,
    )
